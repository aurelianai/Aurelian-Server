services:
  backend:
    build: ./backend
    ports:
      - "2140:2140"
    environment:
      PG_HOST: postgres
      PG_USER: postgres
      PG_PASS: postgres
      PG_PORT: "5432"
      PG_DB: postgres
      JWT_SECRET: "SuperSecretKey"
      INFERENCE_BACKEND: ${INFERENCE_BACKEND}
      USER_CREATE_ACCESS_KEY: "givemeaccess!"
    volumes:
      - ./backend:/backend
      - go_modules:/go
    depends_on:
      - postgres
      - inf
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:2140/health" ]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 5s

  frontend:
    build: ./frontend
    volumes:
      - ./frontend/:/frontend
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5173/health" ]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 5s

  nginx:
    image: nginx:latest
    ports:
      - "8080:8080"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - frontend
      - backend

  postgres:
    image: postgres:latest
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data

  inf:
    image: ghcr.io/huggingface/text-generation-inference:latest
    entrypoint: "text-generation-launcher --model-id MBZUAI/LaMini-GPT-124M"
    shm_size: "1gb"
    ports:
      - 4000:80
    volumes:
      - hf_data:/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:4000/health" ]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 5s

volumes:
  pg_data:
  hf_data:
  go_modules:
