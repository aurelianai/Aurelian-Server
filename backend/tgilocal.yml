model:
  userPrefix: "<human>: "
  userPostfix: "\n"
  modelPrefix: "<bot>: "
  modelPostfix: "\n"
  contextSize: 4096

inference:
  backend: "text-generation-inference"
  endpoint: http://inf/generate_stream
  maxNewTokens: 50
